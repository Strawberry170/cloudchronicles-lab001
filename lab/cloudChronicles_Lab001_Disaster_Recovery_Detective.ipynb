{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1e0979",
   "metadata": {},
   "source": [
    "# üß† cloudChronicles Lab #001: Disaster Recovery Detective\n",
    "\n",
    "**Lab Type:** Idea  \n",
    "**Estimated Time:** 30‚Äì45 mins  \n",
    "**Skill Level:** Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by printing your name to personalize the notebook\n",
    "your_name = \"\"\n",
    "print(f\"Welcome to the lab, {your_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94d019",
   "metadata": {},
   "source": [
    "## üîç STAR Method Lab Prompt\n",
    "\n",
    "**Situation:**  \n",
    "A regional outage has occurred in the us-central1 region of Google Cloud, leading to service disruptions and potential downtime for critical services. The outage impacts several key services, including databases, storage, and messaging systems. As a result, users and applications are experiencing service degradation or loss of access to vital resources.\n",
    "\n",
    "**Task:**  \n",
    "The goal is to ensure business continuity by quickly detecting the regional outage, failing over critical services to a secondary region, and minimizing the impact on the application and end-users. The task is to:\n",
    "\n",
    "- Detect the outage in the us-central1 region.\n",
    "- Fail over key services (e.g., databases, file storage, messaging) to a secondary region.\n",
    "- Recover the failed services with minimal downtime and data loss.\n",
    "- Inform stakeholders and users about the recovery status.\n",
    "\n",
    "**Action:**  \n",
    "1. **Detecting the Outage**\n",
    "   - **Cloud Monitoring & Pub/Sub Alerts**: Set up Google Cloud Monitoring to watch for any signs of regional unavailability or performance degradation. These can include high latency, service unavailability, or dropped requests in the affected region. Configure Pub/Sub to trigger real-time alerts to administrators and other stakeholders when the region goes down.\n",
    "   - **Automated Alerting**: Configure Pub/Sub to send notifications to Slack, email, or a third-party incident management tool such as PagerDuty or Opsgenie, allowing the DevOps team to quickly assess and take action.\n",
    "\n",
    "2. **Failover to Secondary Region**\n",
    "   - **Cloud SQL Replicas**:\n",
    "     - For managed databases like Cloud SQL, ensure that there is an automatic failover setup using Cloud SQL replicas in a secondary region (e.g., us-east1 or europe-west1). This will provide high availability and data replication, ensuring that a secondary database will take over in case of an outage in us-central1.\n",
    "     - **Failover Process**: Once the outage is detected, the application automatically connects to the secondary database, minimizing downtime.\n",
    "     - Use Cloud SQL failover replicas configured with automatic failover or manual failover as necessary.\n",
    "\n",
    "   - **Multi-Region Cloud Storage**:\n",
    "     - For file storage and object data, implement multi-region Cloud Storage. This ensures that your files (e.g., images, videos, backups) are replicated in multiple regions. When a region is unavailable, the application can access the objects from a functioning region, ensuring data availability.\n",
    "     - **Failover Process**: Set up Cloud Storage to automatically read from another region if us-central1 is down. This can be done using the Google Cloud Storage Transfer Service or configuring the Storage Bucket to span multiple regions.\n",
    "\n",
    "   - **Pub/Sub Messaging**:\n",
    "     - Pub/Sub is used for decoupling services in a cloud-native architecture. To ensure messaging services are unaffected by the regional outage, create global Pub/Sub topics and subscribers, meaning messages will continue to be routed even if us-central1 is affected.\n",
    "     - Set up automatic retry logic in subscribers to ensure that messages are not lost if they fail to be processed in the primary region.\n",
    "\n",
    "3. **Disaster Recovery Testing**\n",
    "   - **Backup Procedures**: Ensure that Cloud SQL and Cloud Storage backups are enabled in the secondary region. Regular testing should be scheduled to validate that the failover process works as expected and data integrity is maintained.\n",
    "   - **Manual Failover Drills**: Perform periodic disaster recovery drills in which manual failover is simulated to ensure operational readiness.\n",
    "\n",
    "4. **Post-Recovery Actions**\n",
    "   - **Incident Postmortem**: Once recovery is complete, conduct an incident postmortem. Review the failure event, failover execution, and recovery process. Adjust monitoring, alerting thresholds, and failover strategies as needed.\n",
    "   - **Stakeholder Communication**: Using Cloud Pub/Sub or a dashboard, communicate the recovery status to all relevant stakeholders. Ensure that business operations are restored and provide an estimate for full regional recovery.\n",
    "   - **Performance Monitoring**: Once failover is complete, monitor the performance in the new region to ensure that all services are operating optimally. Use Cloud Monitoring and Cloud Logging to analyze the recovery performance and resolve any issues.\n",
    "\n",
    "**Expected Result:**  \n",
    "By implementing this disaster recovery plan:\n",
    "- **Minimized Downtime**: Service disruptions were minimized by failing over critical services (databases, storage, messaging) to a secondary region with minimal data loss. Cloud SQL replicas ensured automatic failover, and multi-region Cloud Storage provided continuous access to objects.\n",
    "- **Increased Reliability**: The use of multi-region Cloud Storage and global Pub/Sub ensured that even in the event of a regional outage, data and messaging services remained operational, reducing the risk of data loss and service unavailability.\n",
    "- **Real-time Alerts**: By integrating Pub/Sub with monitoring tools, administrators were notified in real-time of the outage, allowing a swift response.\n",
    "- **Scalable Recovery**: As the failover is automated, scaling and recovery operations were smooth and consistent, ensuring business continuity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b221d",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Your Assignment\n",
    "\n",
    "_Use this section to complete your deliverable:_\n",
    "\n",
    "```markdown\n",
    "(Example Format)\n",
    "\n",
    "- **Primary Region**: us-central1  \n",
    "- **Backup Location**: us-east1  \n",
    "- **Failover Trigger**: Load balancer health check + Pub/Sub alert  \n",
    "- **Redundancy Services**:  \n",
    "   - Cloud SQL with failover  \n",
    "   - Cloud Storage versioning  \n",
    "   - Cloud Functions for health monitoring  \n",
    "- **Backup Schedule**: Every 6 hours, daily export to multi-region bucket  \n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
